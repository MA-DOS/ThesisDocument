\section{Approach}
\label{cha:approach}
% TODO: Add references to the sections and prior chapters.
The following section systematically outlines the methodological approach of this thesis. While concrete implementation details and technology-specific decisions are discussed in the subsequent chapter, this section focuses on establishing the theoretical foundation that builds upon the concepts introduced in the background. The proposed research problem is addressed through a threefold decomposition. First, a data collection phase captures detailed execution metrics from scientific workflow runs, as described in Chapter 2. Second, this collected data undergoes in-depth analysis to identify relevant performance characteristics and relationships. Finally, the insights derived from this analysis are employed in the simulation and algorithmic modeling phase, where various co-location strategies are evaluated to study their effects on performance and energy efficiency.

Based on this threefold design, the structure of this chapter is organized as follows. After formally defining the problem statement, a general overview of the methodological approach adopted in this work is presented. Subsequently, a set of assumptions is introduced to clearly delimit the scope, applicability, and limitations of the proposed approach. The chapter then begins with the discussion of online scientific workflow task monitoring, detailing how execution data is collected and structured for further analysis. This is followed by an in-depth explanation of the data analysis phase, focusing on matching task entities from different monitoring sources and leveraging this unified data for statistical exploration. The analysis section begins with embedding methodologies and supervised learning, encompassing data preprocessing and predictive modeling techniques. Thereafter, the focus shifts to unsupervised learning, specifically addressing task clustering as applied in this work. The chapter concludes with a detailed presentation of the theoretical approach to the simulation environment, outlining the main conceptual framework, heuristic design principles for scientific workflow scheduling, the definition of baseline algorithms, and the algorithms devised to implement the novel online co-location strategies developed in this thesis.

\subsection{Problem Statement}
\label{sec:problem_statement}
The central objective of this work is inspired by the design proposed in \cite{5644899}, where task co-location is formulated as a consolidation and clustering problem within a virtualized computing environment. The goal is to consolidate workflow tasks—subject to the structural constraints imposed by the workflow’s Directed Acyclic Graph (DAG)—onto virtualized machines in such a way that their resource usage profiles complement each other, thereby achieving energy-aware execution. While \cite{5644899} approaches this problem statically, determining task clusters and node assignments prior to workflow execution, this thesis extends the problem to a dynamic, online setting. Here, co-location decisions are made during workflow execution, allowing the system to adapt to evolving runtime conditions.

In contrast to the static mapping-based approach, this work integrates co-location directly into the task mapping and scheduling process, arguing that co-location and scheduling are inherently interdependent and should be addressed within a unified framework rather than as separate optimization problems. Consequently, the formulated problem becomes an online co-location problem, where workflow tasks must be characterized before execution in order to enable contention-aware co-location decisions at runtime. The co-location in this context operates at the virtual container level, specifically focusing on virtual machines hosted on physical servers, while contention effects between virtual machines themselves are considered beyond the scope of this thesis.

\subsection{Overview}
\label{sec:overview}
% TODO: Add a general figure, maybe with numbers showing the steps of the overall approach.


\subsubsection{Assumptions}
\label{sec:assumptions}
To clearly outline the scope, boundaries, and methodological constraints of this work, the following guiding assumptions were defined to facilitate this first iteration of research on dynamic scientific workflow co-location:

\begin{enumerate}
    \item Monitoring Configuration Limits: A maximum of 50 monitoring features per workflow task is imposed to ensure manageable execution times and allow for statistical evaluation across varying monitoring configurations. This restriction also underscores the need for future work to investigate the influence of monitoring data quality and dimensionality on predictive performance.
    \item Monitoring Data Quality and Coverage: Not all low-level monitoring capabilities are fully exploited. Instead, existing monitoring data sources are leveraged with minimal modifications to improve compatibility with the monitoring client. Short-lived tasks (typically under one second) are only partially captured or occasionally missed due to system load and sampling intervals exceeding one second.
    \item Offline Data Analysis: All data preprocessing, model training, hyperparameter tuning, and fitting are performed offline after workflow execution. The resulting trained models are then transformed into a suitable format for integration into the simulation environment.
    \item Simulation Environment and Platform Equivalence: The simulated platform is assumed to approximate the physical execution environment despite potentially having more nodes of identical configuration. It is expected that the overall behavior observed in simulation aligns with real-world execution trends.
    \item Simulation Capabilities and Contention Modeling: The WRENCH framework, built on SimGrid, currently supports simulation of memory contention by limiting per-VM memory consumption, where exceeding the limit results in extended task execution times. Similarly, CPU contention is modeled through proportional increases in task runtime. Other low-level contention effects (e.g., cache, interconnect, or I/O contention) are not modeled in this iteration.
    \item Energy Model Assumptions: The energy model provided by SimGrid is assumed to realistically approximate energy consumption variations when tasks with differing resource usage profiles are colocated on the same virtual machine. The strongest impact on energy efficiency is attributed to CPU utilization behavior.
    \item Evaluation Focus: Co-location efficiency is evaluated primarily through per-host energy consumption over time, total workflow energy usage, and overall makespan reduction, which serve as the main indicators of effective virtual machine co-location.
\end{enumerate}

\subsection{Online Task Monitoring}
\label{sec:online_task_monitoring}

% TODO: Add table from towards monitoring here with mentioning the different exporters that were tried and briefly describe how they work with simple enumeration.
The online task monitoring approach developed in this work builds on a hierarchical monitoring architecture that captures the full spectrum of metrics relevant to the execution of scientific workflows. The design follows a four-layered structure consisting of the Resource Manager, Workflow, Machine, and Task layers. Each layer represents a distinct abstraction level in the workflow execution environment and focuses on monitoring its respective component while retrieving necessary metrics from the layers beneath it. This hierarchical organization ensures that monitoring is both scalable and context-aware, enabling adaptive scheduling and subsequent co-location decisions.

The approach deliberately avoids a user-centric design, instead aligning the monitoring structure with the operational hierarchy of distributed workflow execution systems. At the top, the Resource Manager layer provides coarse-grained monitoring information related to cluster status, resource allocation, and job management. The Workflow layer operates on the logical workflow representation, maintaining execution progress, task dependencies, and overall runtime statistics. Below, the Machine layer captures system-level performance data such as CPU, memory, and storage utilization, as well as hardware-specific configurations. At the lowest level, the Task layer delivers fine-grained, time-resolved monitoring data, including per-task resource consumption, low-level kernel metrics, and execution traces.

In this layered structure, each upper layer can access data from its subordinate layers but depends only on a subset of their metrics to operate. For example, the Resource Manager relies on summarized data from the Workflow and Machine layers—such as node availability, workflow DAG progress, and per-task resource utilization—without needing low-level system calls or detailed task traces. The hierarchical data flow thus facilitates aggregation and abstraction of monitoring information, ensuring that higher-level components can make informed decisions without being burdened by excessive data granularity \cite{Bader_2022}.

% Scaphandre, docker-activity, cadvisor, slurm-exporter, ebpf-energy-monitor
Scaphandre is a power monitoring tool designed to measure and attribute energy consumption across computing systems down to the process level. It integrates hardware-based sensors—most notably Intel’s RAPL technology—with system-level resource monitoring to provide fine-grained insights into power usage. By combining energy counters from CPUs and GPUs with resource utilization metrics such as CPU load and memory usage, Scaphandre correlates hardware activity with power consumption in real time. Its operation relies on process-level tracking over short scheduling intervals (jiffies), allowing it to estimate the energy consumed by individual processes or containers.
Through its exporter modules, Scaphandre exposes standardized metrics for external monitoring systems such as Prometheus. Key metrics include total host power and energy consumption (scaph_host_power_microwatts, scaph_host_energy_microjoules), per-process power usage (scaph_process_power_consumption_microwatts), and CPU frequency data. This makes it suitable for energy-aware workload management and sustainability analysis. Scaphandre’s measurement approach leverages Linux’s powercap subsystem for RAPL-based readings and supports virtualized environments, enabling accurate monitoring across VMs and containers. While primarily focused on CPU-level energy attribution, future developments aim to extend its capabilities toward GPU and storage device power tracking to achieve a more comprehensive view of system-wide energy consumption \cite{CENTOFANTI2024110371}.

cAdvisor (Container Advisor) is an open-source daemon for monitoring resource usage and performance of containers (focus here on Docker). It runs with root privileges, exposes a web UI and HTTP APIs (including Prometheus metrics), and continuously discovers containers via Linux cgroups under /sys/fs/cgroup. Once started, cAdvisor initializes in-memory storage, a central Manager, and HTTP handlers, then recovers the container hierarchy and begins housekeeping. Discovery and lifecycle tracking rely on inotify: the raw watcher subscribes to create/delete events in the cgroup filesystem, converts them to internal add/remove events, and (re)configures per-container handlers. Metrics originate from multiple layers: machine-level facts (CPU model/clock, memory, disks, NICs) parsed from /proc and /sys; container/process usage (CPU, memory, I/O, network) collected at cgroup boundaries; and optional accelerator/GPU hooks. The architecture comprises a cache, event channels, container watchers, per-runtime plugins (Docker, containerd, CRI-O, systemd), and HTTP handlers for both JSON APIs (e.g., /api/v1.0/containers) and raw metric endpoints. The web UI surfaces machine/FS status, running processes, and per-container time series (CPU, memory, disk, network). Programmatically, a Go client can query container specs, stats, events, and machine info. In practice, cAdvisor provides low-overhead, per-container telemetry suitable for local inspection or integration into observability stacks (e.g., Prometheus), with discovery via cgroups, change detection via inotify, and rich system context via /proc//sys. \cite{Tolaram2023}.

% TODO: Cite github project.
Docker Activity is an early-stage, Rust-based container telemetry tool that augments Docker stats with per-container energy estimates. It requires access to the Docker engine socket and to /sys/class/powercap to read Intel/AMD RAPL counters; thus it currently supports power reporting only on RAPL-compatible CPUs. At runtime an orchestrator (using the Bollard Docker client) lists running containers, subscribes to Docker events, and spawns a lightweight watcher per container that streams docker stats‐like samples. Each sample is mapped into a Record containing container ID/name, timestamp, CPU usage (computed from deltas of total vs. system CPU time and normalized by the online CPU count), PIDs, and memory usage/limits; when the optional enrichment-powercap feature is enabled, the tool reads package energy and attributes a share to the container by scaling total CPU energy with that container’s instantaneous CPU fraction. Records are passed over an async channel to a pluggable exporter (e.g., JSON/Prometheus in future), enabling integration into monitoring stacks. In short, Docker Activity provides process/container-level observability with coarse energy attribution via RAPL, suitable for exploratory energy profiling of containerized workloads under realistic permissions and without external power meters.

The intuition behind DEEP-mon’s per-thread power attribution method is to translate coarse-grained hardware power measurements into fine-grained, thread-level energy estimates by exploiting hardware performance counters. The Intel RAPL interface provides power readings per processor package or core, but it cannot distinguish how much of that energy was consumed by each thread. DEEP-mon bridges this gap by observing how actively each thread uses the processor during each sampling interval. It does so by monitoring the number of unhalted core cycles—a counter that measures how long a core spends executing instructions rather than idling. Since power consumption correlates almost linearly with unhalted core cycles, the fraction of total cycles attributed to each thread provides a reasonable proxy for its share of energy usage.
A key refinement of this method concerns Hyper-Threading (HT), where two logical threads share the same physical core. When two threads co-run on the same core, they do not each consume as much power as if they were running on separate cores. Experimental evidence shows that a physical core running two logical threads consumes about 1.15× the power of a single-threaded core. DEEP-mon incorporates this observation through a weighting factor (HTr), which adjusts the power attribution depending on whether a thread was running alone or sharing a core. If a thread runs concurrently with another on the same core, its share of the power is scaled down accordingly and divided equally between the two.
In essence, DEEP-mon first computes the “weighted cycles” for each thread—combining its active cycles when alone with its proportionally reduced cycles when co-running. These weighted cycles determine how much of the total core-level RAPL energy should be assigned to that thread. The final per-thread power estimate is then derived by distributing the total measured power of each socket proportionally to the weighted cycle counts of all threads running on that socket. This approach allows DEEP-mon to infer realistic thread-level power usage even in systems with simultaneous multithreading and time-shared workloads, without modifying the scheduler or requiring any application-specific instrumentation.
The intuition behind DEEP-mon’s per-thread power attribution method is to translate coarse-grained hardware power measurements into fine-grained, thread-level energy estimates by exploiting hardware performance counters. The Intel RAPL interface provides power readings per processor package or core, but it cannot distinguish how much of that energy was consumed by each thread. DEEP-mon bridges this gap by observing how actively each thread uses the processor during each sampling interval. It does so by monitoring the number of unhalted core cycles—a counter that measures how long a core spends executing instructions rather than idling. Since power consumption correlates almost linearly with unhalted core cycles, the fraction of total cycles attributed to each thread provides a reasonable proxy for its share of energy usage.
A key refinement of this method concerns Hyper-Threading (HT), where two logical threads share the same physical core. When two threads co-run on the same core, they do not each consume as much power as if they were running on separate cores. Experimental evidence shows that a physical core running two logical threads consumes about 1.15× the power of a single-threaded core. DEEP-mon incorporates this observation through a weighting factor (HTr), which adjusts the power attribution depending on whether a thread was running alone or sharing a core. If a thread runs concurrently with another on the same core, its share of the power is scaled down accordingly and divided equally between the two.
In essence, DEEP-mon first computes the “weighted cycles” for each thread—combining its active cycles when alone with its proportionally reduced cycles when co-running. These weighted cycles determine how much of the total core-level RAPL energy should be assigned to that thread. The final per-thread power estimate is then derived by distributing the total measured power of each socket proportionally to the weighted cycle counts of all threads running on that socket. This approach allows DEEP-mon to infer realistic thread-level power usage even in systems with simultaneous multithreading and time-shared workloads, without modifying the scheduler or requiring any application-specific instrumentation \cite{8425477}.

% TODO: Insert algorithm of the monitoring client here.
% TODO: Insert schematic overview of the monitoring client here.
% TODO: Insert  some examples here on how the collected data looks or maybe better in evaluation.

\subsection{Data Analysis}
\label{sec:data_analysis}
The monitoring data collected during workflow execution was systematically stored in a structured results directory to facilitate reproducible analysis and efficient data access. Each monitoring dimension—CPU, memory, disk, network, and energy—was written into separate subdirectories under a common results path. Within these directories, data were further organized according to the monitoring source or tool, allowing a clear separation of heterogeneous data types and measurement granularities. For example, the task_cpu_data directory contains measurements obtained from the eBPF-based monitoring component, which captures per-container cycle-level CPU activity. This data is stored in the subdirectory ebpf-mon/container_weighted_cycles/ as a single CSV file named container_weighted_cycles.csv. Comparable directory structures exist for memory, disk, energy, and network metrics, each containing the respective monitored variables for every task executed during the workflow.

In addition to these resource-specific folders, global logs such as started_nextflow_containers.csv and died_nextflow_containers.csv were generated to record container lifecycle events. These logs serve as temporal anchors for correlating resource consumption data with workflow execution phases. Together, this hierarchical organization provides a unified and extensible structure for data analysis, ensuring that measurements from different monitoring layers and time intervals can be efficiently queried and merged in subsequent analysis steps.
\subsubsection{Task Entity Matching}
\label{sec:task_entity_matching}
% Mention approach is based on workflow monitoring paper
% Insert sketch inspired by the workflow monitoring paper that shows entity matching examples
The first phase of the data analysis focuses on entity matching—the systematic alignment of heterogeneous monitoring data sources into a unified representation of each executed workflow task. During workflow execution, diverse monitoring tools and system components produce data at different abstraction levels, ranging from container-level resource traces to process-level identifiers and workflow-specific metadata. To enable integrated analysis, these fragmented data streams must be correlated and matched against the task entities defined by the workflow management system (in this case, Nextflow).

The matching process begins with the Nextflow trace and container lifecycle records (started_nextflow_containers.csv and died_nextflow_containers.csv), which provide information on task identifiers, container names, process hashes, and working directories. These files serve as the primary link between workflow-level entities and low-level monitoring data. The analysis framework first scopes the full results directory to isolate only the relevant monitoring sources defined in the configuration file (config.yml). This configuration specifies which monitoring targets, data sources, and power metering tools were active during execution, thereby reducing the data space to those sources that are semantically compatible for integration.
Next, each monitoring source (e.g., CPU, memory, energy) is traversed recursively to locate and load the associated time-series data. These data files are organized hierarchically under source-specific directories and often contain multiple interleaved measurements from different tasks. To facilitate task-level analysis, the pipeline splits these aggregated time-series datasets into per-task CSV files using source-specific identifiers (for instance, container names or task hashes). The splitting procedure ensures that all subsequent analysis can be performed at the level of individual workflow tasks.
Afterward, a consistency check is performed across all monitoring sources to identify tasks that appear in some data streams but not others. This step reports missing or unmatched entries and verifies that each monitored task can be traced across the CPU, memory, disk, and energy datasets. Once verified, each per-task dataset is enriched with contextual metadata. The first enrichment phase attaches the correct working directory to every container trace, allowing file system references to be used as stable task anchors.
The next stage introduces workflow-level semantics by matching the per-task monitoring data with the workflow’s own metadata. Using the exported Nextflow trace file (trace.txt), the analysis extracts task names and their corresponding working directories. These entries are matched against container-level records from the monitoring data to identify which monitored container corresponds to which logical task in the workflow. The resulting mapping is used to update all per-task monitoring files by appending the correct Nextflow process name.

\subsection{Statistical Embedding and Supervised Learning}
\label{sec:statistical_embedding_and_supervised_learning}
% TODO: Add intro sentences based on general supervised learning and embedding techniques and mention that it's mostly based on the power aware paper.
\subsubsection{Data Preprocessing}
\label{sec:data_preprocessing}
% TODO: Add formal notation of the feature matrices
We transform the heterogeneous, time‐stamped monitoring traces into consistent task-level feature/label matrices suitable for statistical learning and KCCA. The pipeline proceeds in four steps: scoping & harmonization, per-task time-series extraction, signature construction, and label assembly & alignment.
\begin{enumerate}
    \item Sampling & smoothing. 
    \item Per-task time-series extraction.
    \item Temporal signature construction (feature selection)
    \subitem Sampling and smoothing.
    \subitem Equal-length normalization.
    \subitem Container-wise collation.
    \item Label assembly & alignment.
    \subitem Power label extraction.
    \subitem Runtime normalization.
    \subitem Y matrix assembly.
    \subitem Index alignment.
\end{enumerate}

This preprocessing yields: (i) a standardized, fixed-length, noise-robust feature matrix X that preserves per-metric usage distributions; (ii) a label matrix Y capturing runtime and energy; and (iii) a clean, task-aligned index. Together, these artifacts support downstream supervised modeling (e.g., regression, KCCA) and unsupervised analyses (e.g., clustering) without additional wrangling.

\subsubsection{Predictive Modeling}
\label{sec:predictive_modeling}

In the final stage of the analysis pipeline, the collected and preprocessed task data were prepared for multiview learning. The goal was to learn relationships between the task-specific feature representations and the corresponding performance and energy outcomes. To enable this, the data were divided into training and testing subsets, ensuring that roughly 70% of the tasks were used for model training and the remaining 30% for validation.

Before training, both the feature data (X) and the target data (Y)—consisting of task runtime and energy consumption—were standardized independently. Each dataset was centered to a mean of zero and scaled to unit variance. This normalization step was necessary because the raw features and target values operate on very different numerical ranges, and kernel-based methods are highly sensitive to such scale differences. Standardization ensured that all variables contributed equally to the learning process and prevented features with large absolute values from dominating the optimization.
\paragraph{Kernel Canonical Correlation Analysis (KCCA)}
\label{sec:kcca}
Following normalization, a Kernel Canonical Correlation Analysis (KCCA) model was trained to uncover shared structures between the feature space and the target space. In essence, KCCA finds correlated projections of both datasets onto a common latent space, allowing the model to identify nonlinear relationships between resource usage patterns and energy or runtime behavior. To determine the most suitable kernel function for this dataset, several alternatives such as linear, cosine, radial basis function (RBF), Laplacian, and polynomial kernels were evaluated using cross-validation. The kernel that maximized the inter-view correlation on the training data was selected for further use.
Once the KCCA model was fitted, it was extended into a predictive framework. The latent representation learned from the feature data was used to train a simple linear regression model that maps the learned feature embeddings to the corresponding runtime and energy targets. This enabled the model not only to describe correlations but also to predict task outcomes directly from unseen feature data.
Finally, the predictive performance was evaluated on the test data by comparing the model’s estimates against the actual runtime and energy measurements. The evaluation relied on metrics such as mean squared error (MSE) and the coefficient of determination (R²), which together quantify how well the model explains variability in the observed data and how accurately it generalizes to new tasks. Through this process, the preprocessing and multiview modeling steps provided a systematic way to connect measured system behavior to its energy and performance implications.

\paragraph{Random Forest Regression}
\label{sec:random_forest_regression}
To complement the multiview model, we trained two non-parametric regressors based on Random Forests—one to predict mean task power and one to predict task runtime from the same preprocessed feature matrix. As a sanity check, we established simple baselines by predicting the training-set mean of the target (once for power, once for runtime) on the test split and reporting the corresponding absolute errors. These baselines quantify the minimum improvement any learned model must exceed.
For each target, we split the data into training and test partitions (≈70/30). We then fit a RandomForestRegressor on the training set only, using the task feature vectors as inputs and either mean power or runtime as the scalar target. Because tree ensembles are scale-invariant, no additional normalization of X was required beyond the preprocessing used to build the feature matrix; targets were kept in their physical units to preserve interpretability.
Model capacity was tuned with randomized hyperparameter search under K-fold cross-validation. The search space spanned the number of trees, maximum tree depth, feature subsampling at split time, minimum samples per split/leaf, sampling fraction per tree, and splitting criterion. This procedure balances bias–variance trade-offs, encourages decorrelated trees through feature subsampling, and mitigates overfitting via depth and sample constraints. The best configuration (by cross-validated error on the training set) was then refit on the full training partition.
We evaluated generalization on the held-out test set using: (i) mean absolute error (MAE) to report average deviation in natural units; (ii) mean absolute percentage error (MAPE) to provide a scale-free notion of accuracy; and (iii) coefficient of determination (R²) to quantify explained variance. We compared these metrics directly to the mean-predictor baseline to show absolute and relative gains. For transparency, we also report the model’s score (R²) from scikit-learn, which matches our R² computation.
When predicting power, the forest was trained against the mean per-task energy-rate labels and scored on the test split; an analogous model was trained for runtime. Hyperparameters were tuned separately for each target, as optimal depth, tree count, and sampling often differ between the (potentially noisier) energy signal and runtime. Finally, because forests offer native feature-importance measures, the trained models can be probed to identify which components of the task signatures contribute most to power vs. runtime predictions—useful later for scheduler heuristics and for validating that the learned signals align with domain expectations.

\subsection{Unsupervised Learning}
\label{sec:unsupervised_learning}
% TODO: Write a couple sentences about where the clustering approach comes from, what the idea behind it is based on the power aware power and maybe 2 or three other related works.
\subsubsection{Task Clustering}
\label{sec:task_clustering}
We cast consolidation as a clustering problem with a twist: instead of grouping similar tasks, we deliberately group tasks that are dissimilar in their resource demands so that colocated tasks complement each other and reduce contention. To do this, we build a task–task distance that grows when two tasks exercise the same resource in the same way, and shrinks when their peak-usage patterns are complementary.

\begin{enumerate}
    \item Peak-pattern construction. For every task and monitored workload type (CPU, memory, file I/O), we first derive a peak time series: the raw per-second resource signal is resampled into three-second buckets and the maximum per bucket is retained. This emphasizes contention-relevant bursts while smoothing short noise spikes. Each series is then normalized onto a relative time axis (seconds since first sample) to make tasks of different absolute start times comparable. When two peak series must be compared, we truncate both to the shorter length (or interpolate to a common grid when needed) so that correlation is computed on aligned vectors without padding artifacts.
    \item Workload-type affinity. Different resource domains interfere to different degrees (e.g., CPU vs CPU peaks are typically more contentious than CPU vs file I/O). We encode this with an empirical “affinity score” between workload types (cpu–cpu, cpu–mem, mem–io, etc.). High affinity means higher potential interference when peaks align; low affinity reflects benign coexistence.
    \item Anti-similarity distance. For any pair of tasks i,j, we iterate over their workload types and compute two ingredients: (i) the affinity between the two types; (ii) the correlation between their corresponding peak series (computed twice, once per type, to capture both sides of the pairing). We then aggregate these terms so that highly correlated peaks in high-affinity domains increase the distance, whereas low or negative correlations in low-affinity pairs decrease it. The result is a symmetric task–task distance matrix whose off-diagonal entries quantify “how bad” it would be to co-locate the two tasks, and whose diagonals are zero by definition.
    \item Sanity filtering. Some tasks (e.g., very short-lived or purely memory-resident ones) may produce constant peak series, which makes correlation ill-defined. We detect and drop such pathological series from distance estimation for robustness; this prevents NaNs from contaminating the matrix.
    \item Threshold selection. Because the distance matrix is data-dependent, we estimate a merge threshold directly from its empirical distribution (lower triangle, excluding the diagonal). A quantile (e.g., the 40th percentile on the raw distances) acts as an automatic cut-level: any pair below this threshold is “safe enough” to consider for co-location, while pairs above it are kept apart. This adaptive choice avoids brittle, hand-tuned cutoffs.
    \item Agglomerative clustering with precomputed distances. We run average-linkage agglomerative clustering on the precomputed distance matrix with the chosen distance threshold and no preset cluster count. This yields variable-sized clusters whose members are mutually non-contentious under our metric. Because we use a threshold rather than a fixed k, the method adapts to each workload mix, producing more or fewer groups as warranted by the observed interference structure.
    \item From clusters to co-location candidates. Each cluster defines a candidate co-location set. To make these cluster-level entities usable by downstream predictors (e.g., KCCA or Random Forest), we construct cluster feature vectors by flattening and concatenating the per-task temporal signatures (pattern vectors for CPU/memory/file I/O) of all members and summing them dimension-wise. This simple, permutation-invariant aggregation approximates the combined load shape the scheduler would see if the cluster’s tasks were run together on the same host. (In future iterations, this can be refined to weight members by peak overlap fraction and dwell time in peak regions.)
\end{enumerate}

\subsection{Measuring Resource Contention}
\label{sec:measuring_resource_contention}

% Write that the inspiration comes from the power aware paper. Reference the distance formula from previous sections and say that the affinity matrix is based on empirical observations. Mention that the results will be shown in the Chapter 6.
% Affinity calculation notation needs to come here.
The measurement of resource contention follows a two-stage protocol that first establishes isolated baselines for each workload class and then repeats the same workloads under controlled co-location. In the baseline stage, CPU-bound, memory-bound, and file-I/O-bound containers are executed one at a time on pinned logical CPUs. Pinning fixes placement and removes scheduler noise; for I/O experiments the file set is prepared once and cleaned afterward to avoid warm-cache artifacts. Each run records a start and finish timestamp at microsecond resolution and derives the wall-clock duration. In parallel, the monitoring pipeline supplies per-container power time series. After a run finishes, only the power streams belonging to the participating container are retained, aligned to the container’s lifetime, and summarized to a representative mean value.
The co-location stage replays the same workloads in pairs to expose interference effects. Pairs are chosen to cover both homogeneous combinations, where both containers stress the same resource class, and heterogeneous combinations, where their dominant pressure differs. Placement again uses CPU pinning. Some experiments bind pairs to siblings on the same physical core to amplify shared-core effects; others place them on distinct cores to isolate memory bandwidth or storage contention. Each co-located container is measured in exactly the same way as in isolation, producing a matched set of durations and power summaries.
From these measurements, contention is characterized by comparing the co-located outcomes against the isolated baselines for the same workloads. For each pair, the procedure derives how much slower the workloads ran together relative to alone and how their average power changed. To avoid overfitting to any single signal, runtime and power effects are aggregated into a single scalar that captures the overall quality of the pairing. Values above a neutral threshold indicate that the pair “plays well together,” while values below it signal destructive interference. This single number is what the co-location policies use downstream: it serves both as the supervision signal for learning-based components and as the ground truth to validate scheduling heuristics in simulation.
Two practical details make the pipeline robust. First, container identity and workload class are tracked via labels rather than ephemeral names, which keeps joins between execution logs and monitoring traces unambiguous. Second, exporter data are pruned to the containers actually involved in an experiment, and timestamps from different subsystems are normalized before aggregation. Together, these steps ensure that any observed performance or energy shift can be attributed to co-location rather than to measurement artefacts or incidental scheduler behavior.
\subsection{Simulation Environment}
\label{sec:simulation_environment}

\subsubsection{Design Pillars}
\label{sec:design_pillars}
The simulator for co-location strategies builds upon three fundamental design pillars that reflect the main optimization opportunities identified in the co-scheduling problem: resource allocation, queue ordering, and job placement. The simulator aims to reproduce these decision dimensions in a controlled environment, allowing systematic evaluation of co-scheduling strategies under varying workload and system conditions.

The first pillar concerns resource allocation, which determines whether jobs are executed in exclusive or shared mode. Traditional HPC schedulers allocate full nodes to single jobs, but the co-scheduling paradigm assumes that multiple applications can coexist efficiently if they do not saturate the same resources simultaneously. The simulator therefore models node-sharing policies where jobs may share cores, memory bandwidth, or caches depending on their resource profiles.

The second pillar addresses queue ordering and dispatching, which influence throughput and fairness. Since the effectiveness of co-scheduling depends on the characteristics and arrival times of jobs, the simulator explores alternative queue management strategies—ranging from conventional FIFO ordering to heuristic reordering that prioritizes beneficial pairings. This enables analysis of trade-offs between system-level metrics, such as makespan and utilization, and user-oriented metrics, such as slowdown or turnaround time.
The third pillar focuses on job placement within available resources, emphasizing how specific pairings or groupings affect performance. The simulator integrates the concept of pair matching, as identified in the paper, where dissimilar workloads are co-located to minimize contention and maximize utilization. It supports both random pairing and heuristic approaches that aim to exploit workload complementarity.
Following the rationale of the original work, the simulator evaluates co-scheduling effectiveness through key performance metrics: makespan speedup, weighted mean job speedup (WMJS), utilization, and mean slowdown. It thereby enables the quantification of trade-offs between performance improvement and fairness. Moreover, the simulator design incorporates the statistical modeling perspective proposed in the paper, allowing the examination of correlations between these metrics to identify conditions where co-scheduling is advantageous.
Finally, the simulator supports experimentation with opportunistic and greedy scheduling strategies, reflecting the pragmatic orientation of the co-scheduler research. Exhaustive optimization is computationally infeasible; hence, the simulator adopts scalable heuristics for dynamic pairing, bulk scheduling, and queue reordering. This design allows evaluation of how localized scheduling decisions affect global system performance and resource efficiency, ultimately enabling controlled exploration of the principles behind practical HPC co-scheduling \cite{unknown}.

\subsubsection{Heuristic Design}
\label{sec:heuristic_design}
% TODO: Write about general workflow scheduling approach and mention classic round, robin, fifo, backfilling and Min-Max Scheduling.
The simulator follows a heuristic-based design philosophy. Instead of formulating the scheduling process as an optimization problem defined by a cost function or a multi-objective fitness model, the approach aims to achieve energy awareness through embedded decision-making within simple, interpretable scheduling and task mapping rules. Heuristic algorithms are well suited for such settings because they rely on deterministic, rule-based traversal of the search space rather than exhaustive or stochastic exploration. They exploit domain-specific knowledge and structured criteria—such as task priorities, resource affinities, or workload complementarities—to produce acceptable solutions efficiently without guaranteeing global optimality.

In this context, the heuristic scheduler is designed to incorporate energy-related considerations directly into basic scheduling steps such as task ordering and resource selection. Rather than seeking a mathematically optimal solution, it focuses on guiding the scheduling process toward energy-efficient outcomes through lightweight decision rules. These rules are derived from observed workload behaviors and co-location characteristics, ensuring that the resulting mappings balance computational load, minimize interference, and reduce redundant energy usage. The heuristic operates deterministically: once a feasible schedule is reached, the process terminates. This design choice provides a practical balance between computational efficiency, interpretability, and responsiveness—qualities that are essential for evaluating co-location strategies under realistic conditions without the complexity of meta-heuristic or multi-objective optimization frameworks \cite{HosseiniShirvani2024}.
Machine learning–based and list-scheduling approaches represent two major paradigms in workflow scheduling. Machine learning–driven scheduling builds on data-driven models that learn decision policies from historical workflow executions or runtime observations. Methods such as deep learning, Q-learning, and reinforcement learning are primarily used to model task execution patterns, predict failures, and adapt scheduling decisions dynamically. For example, the Deep Q-learning Heterogeneous Earliest Finish Time (DQ-HEFT) algorithm integrates reinforcement learning into the classical HEFT framework, using a neural network to model interactions with the cloud environment and to optimize task–processor mappings under cost, time, and budget constraints. Other studies employ supervised models such as decision trees to predict task or job failures based on workload traces from large systems like Google or Alibaba, allowing proactive rescheduling. More advanced models, such as the HunterPlus framework, combine recurrent neural networks (GRUs) with workflow graph inputs to optimize energy efficiency and minimize SLA violations. Generally, machine learning–based scheduling enables adaptive and predictive behavior by continuously refining mappings and execution orders based on performance feedback.
Beyond per-task list-scheduling, we also support cluster-scheduling, which first forms task clusters (to keep heavy communicators together), then merges clusters onto a limited processor set, and finally orders tasks within each cluster. For clustering, we include representatives spanning the design space: linear/critical-path–driven approaches (e.g., LC), dominant-sequence strategies that greedily place a task with its constraining parent and optionally relocate other parents when beneficial (DSC), and dynamic-critical-path variants (DCP) that update allocated levels iteratively and allow “squeezing” a critical task into existing timelines when this does not increase the schedule length. For cluster merging, we provide (a) simple load-balancing (assign largest clusters first to the least-loaded processor), (b) guided load-balancing that merges in order of clusters’ earliest start times to reduce temporal interference, and (c) a list-scheduling adaptation that assigns entire clusters by evaluating the resulting schedule length per processor and picking the best. Finally, intra-cluster task ordering can follow allocated bottom-level (criticality-aware) or a ready-critical-path/ETF-like strategy to suppress idle gaps.
Cluster-scheduling represents a hierarchical approach to task scheduling that first groups interdependent or communication-intensive tasks into clusters before assigning them to processors. It operates in three major phases: clustering, cluster merging, and intra-cluster task ordering. In the clustering phase, algorithms such as Linear Clustering (LC), Dominant Sequence Clustering (DSC), and Dynamic Critical Path (DCP) progressively group tasks based on their critical paths and communication dependencies. LC iteratively extracts the longest remaining path in the task graph, while DSC and DCP extend this idea by incorporating additional heuristics that relocate tasks or dynamically update critical paths to minimize communication delays and start times.
In the cluster-merging phase, the initially unlimited clusters are mapped onto a limited set of processors. The simplest merging method applies load balancing, where clusters are sorted by computational weight and assigned to the least loaded processor. More advanced methods, such as Guided Load Balancing (GLB), consider the temporal alignment of processor loads, merging clusters based on their earliest start times to better reflect real execution overlap. Alternatively, a list-scheduling adaptation can be used, where each cluster is assigned according to the minimal increase in total schedule length estimated from current processor allocations.
Finally, in the task-ordering phase, tasks within clusters are sequenced according to established priority rules from list-scheduling. These can include static metrics such as bottom-level order or dynamic rules such as the Ready Critical Path method, which prioritizes tasks that can start earliest while minimizing idle time. Together, these stages form an integrated scheduling pipeline that balances communication efficiency, parallelism, and processor utilization by combining clustering heuristics with list-based ordering principles.
In contrast, list-scheduling algorithms form one of the most established heuristic approaches for workflow scheduling. These algorithms operate in two stages: first, they assign a priority or ranking to each task based on topological and performance factors (e.g., critical path length, execution cost, or communication overhead), producing a priority list; second, they iteratively select the highest-priority unscheduled task and map it to a processor that minimizes a defined objective function such as earliest finish time. The classical Heterogeneous Earliest Finish Time (HEFT) algorithm exemplifies this approach by ordering tasks based on bottom-level ranking and assigning them to processors that minimize completion time. Numerous variants extend this principle, incorporating multi-objective functions for reliability, energy, or cost optimization. For example, energy-aware list schedulers integrate dynamic voltage and frequency scaling (DVFS) to reduce power consumption, while reliability-aware schedulers replicate critical tasks across multiple processors to enhance fault tolerance. Hybrid methods also exist, such as HH-LiSch or HEFT-TD, which combine ranking heuristics with duplication or insertion strategies to reduce makespan and improve resource utilization \cite{8301529}.


\paragraph{Baseline Algorithms}
\label{sec:baseline_algorithms}
% Explain baselines based on code.
% Baseline 1
% TODO: For baselines mention that all algorithms can be found in the appendix.
The baseline scheduling algorithm implements a simple, sequential execution model designed to simulate isolated task processing within a virtualized cluster. The scheduling process is divided into three abstract components that operate in a fixed order: task scheduling, node assignment, and resource allocation. The scheduler applies a first-in, first-out (FIFO) policy, maintaining a queue of workflow tasks sorted by their readiness. Tasks are retrieved from this queue strictly in order of arrival, preserving dependency constraints and ensuring a fully deterministic execution sequence without reordering or prioritization.
Once a task is selected for execution, the node assignment component distributes it across available compute hosts using a round-robin policy. This mechanism cycles through hosts in sequence, ensuring an even and systematic distribution of tasks across the cluster. No host is assigned more than one active task at a time, enforcing exclusive execution and preventing contention for shared resources.
For each assigned task, the allocator component handles all aspects of resource creation and management. It first determines the file locations of the task’s input and output data, ensuring correct data placement and accessibility. Then, it instantiates a virtual machine on the selected host, configured according to the task’s resource requirements. The task is packaged as a job, submitted to the virtual machine for execution, and monitored until completion. After execution, the allocator triggers the controlled shutdown and destruction of the virtual machine, releasing the allocated resources before moving to the next task.
This composition—FIFO scheduling, round-robin host assignment, and VM-based task allocation—defines a static and reproducible baseline. It excludes adaptive heuristics, predictive optimization, and parallelism, emphasizing clear causality and isolation. The resulting execution model establishes a neutral reference for measuring the impact of more advanced heuristic or co-location-aware scheduling strategies developed later in the work.
% Baseline 2
This variant keeps the same FIFO scheduler and VM-based allocator as Baseline 1, but replaces exclusive node assignment with a greedy backfilling policy. Tasks are still dequeued strictly in arrival order by the FIFO scheduler. For each ready task, the node assignment component queries the cluster for the current number of idle cores per host and performs a first-fit scan: it selects the first host that reports at least one idle core (i.e., idle_cores > 0), without requiring the host to be completely idle. The allocator then provisions a 1-vCPU VM on the chosen host, binds the task’s inputs/outputs, submits the job to that VM, and on completion shuts the VM down and destroys it.
Conceptually, this turns the placement step into gap filling rather than strict exclusivity. Multiple tasks can be co-located on the same host up to its core capacity, increasing instantaneous parallelism and utilization. The policy is intentionally simple: it ignores NUMA/bandwidth considerations, does not rotate the starting host (thus behaving like greedy first-fit rather than load-balanced round-robin), and relies on the compute service to enforce capacity. As a result, Baseline 2 isolates the effect of backfilling co-location under FIFO arrival while keeping resource provisioning and I/O handling identical to Baseline 1.
% Baseline 3
This variant builds upon the same FIFO scheduler but replaces the standard allocator and node assignment with co-location–aware components. Tasks are still dequeued in strict arrival order by the FIFO scheduler. When the node assignment component queries the cluster for idle-core availability, it again selects the first host with available cores. However, instead of launching one VM per task, all ready tasks that fit within the host’s idle-core capacity are grouped into a single batch. These tasks are then co-located inside one shared VM instance that is dimensioned according to the aggregate resource requirements of the batch—its vCPU count and memory size are computed as the sum of the respective task demands.
The allocator provisions this composite VM on the selected host, starts it, and submits each task within the batch as an independent job to the same virtual compute service. The VM remains active until all tasks mapped to it have completed, at which point the allocator shuts it down and destroys it. This mechanism allows multiple tasks to execute concurrently within a shared virtual context while maintaining isolation between hosts.
Conceptually, this baseline captures the behavior of intra-VM co-location, where multiple independent tasks share the same virtual machine instead of being distributed across separate ones. It preserves FIFO task ordering and first-fit host selection but alters the allocation granularity from “one VM per task” to “one VM per batch.” The result is a controlled co-location model that increases per-host utilization while maintaining deterministic scheduling order and consistent provisioning logic.
% Baseline 3.1
This variant builds upon the same FIFO scheduler but replaces the standard allocator and node assignment with co-location–aware components. Tasks are still dequeued in strict arrival order by the FIFO scheduler. When the node assignment component queries the cluster for idle-core availability, it again selects the first host with available cores. However, instead of launching one VM per task, all ready tasks that fit within the host’s idle-core capacity are grouped into a single batch. These tasks are then co-located inside one shared VM instance that is dimensioned according to the aggregate resource requirements of the batch—its vCPU count and memory size are computed as the sum of the respective task demands.
The allocator provisions this composite VM on the selected host, starts it, and submits each task within the batch as an independent job to the same virtual compute service. The VM remains active until all tasks mapped to it have completed, at which point the allocator shuts it down and destroys it. This mechanism allows multiple tasks to execute concurrently within a shared virtual context while maintaining isolation between hosts.
Conceptually, this baseline captures the behavior of intra-VM co-location, where multiple independent tasks share the same virtual machine instead of being distributed across separate ones. It preserves FIFO task ordering and first-fit host selection but alters the allocation granularity from “one VM per task” to “one VM per batch.” The result is a controlled co-location model that increases per-host utilization while maintaining deterministic scheduling order and consistent provisioning logic.

% Baseline 3.2
This variant keeps the FIFO scheduler (tasks are dequeued strictly in arrival order) and the co-location allocator (multiple tasks share a single VM), but replaces the placement policy with a max-idle host selector. At each dispatch, the node-assignment component queries the cluster for the current “idle cores per host” map and picks the host with the largest number of free cores. It then forms a batch by taking as many ready tasks from the FIFO head as the chosen host can accommodate (up to its idle-core count), preserving FIFO order within the batch.
The co-location allocator provisions a single VM on that host sized to the batch’s aggregate demand (vCPUs = sum of task core requirements; memory = sum of task memory requirements), starts it, and submits each task as an independent job to the same VM. The VM remains active until all tasks mapped to it have completed, after which it is torn down. Conceptually, this policy implements best-fit by capacity at the host level (always fill the roomiest host first) combined with intra-VM co-location for the selected batch. Compared to first-fit co-location, it tends to reduce residual fragmentation by packing work onto the most spacious node, while still honoring FIFO ordering and leaving task runtime/I/O handling unchanged.

% Baseline 4
This variant retains the same FIFO scheduler but introduces a node assignment and allocation policy focused on maximizing parallel host utilization. Tasks are still dequeued strictly in arrival order by the FIFO scheduler. Upon each scheduling cycle, the node assignment component queries the cluster for the current number of idle cores per host, filters out fully occupied nodes, and ranks the remaining hosts in descending order of available cores. It then assigns tasks in batches, filling the host with the highest idle capacity first and grouping as many ready tasks as the host’s idle-core count allows. Once the first host is filled, the process continues with the next host until all tasks in the ready queue are mapped.
The allocator provisions one VM per host batch, sizing it to match the aggregate requirements of all tasks assigned to that host. The resulting VM’s vCPU and memory configuration reflect the total core and memory demands of the batch. Each task in the batch is submitted as an independent job to the same virtual compute service, and the VM remains active until all its co-located tasks have finished, at which point it is shut down and destroyed. This ensures that resource lifetime is tied to the collective completion of all tasks sharing the same virtual machine.
Conceptually, this variant extends the intra-VM co-location principle by scaling it across multiple hosts in parallel. It preserves FIFO task ordering but replaces simple first-fit placement with a capacity-ranked packing policy that fills the most capable hosts first. The result is a maximally parallel co-location strategy that maintains deterministic scheduling behavior while improving system-wide resource utilization through host-level batching and VM sharing.

% Baseline 4.1
This variant keeps the FIFO scheduler and the capacity-ranked batching strategy, but adds deliberate oversubscription during co-location. Tasks are still dequeued strictly in arrival order. At each scheduling cycle, the node assignment component queries per-host idle cores, sorts hosts in descending idle capacity, and fills the largest host first. Unlike the non-oversubscribed version, the per-host batch may exceed the currently idle cores by a fixed factor (e.g., 25\%): the batch limit is set to ⌈idle_cores × (1 + oversub_factor)⌉. The procedure continues down the ranked host list, forming one batch per host in the same cycle.
The allocator provisions one VM per host batch, but caps the VM’s vCPU count to the host’s actual idle cores at allocation time (not the sum of task core demands), while sizing memory to the aggregate of the batched tasks. All tasks in the batch are then submitted to that single VM and execute concurrently on a vCPU pool intentionally smaller than their combined declared cores. The VM remains active until all co-located tasks complete, then it is shut down and destroyed.
Crucially, the degree of contention—and thus realized speedup or slowdown—depends on the complementarity of the co-located task profiles. When CPU-, memory-, and I/O-intensive phases overlap unfavorably (e.g., several CPU-bound tasks), oversubscription amplifies interference and queueing on scarce vCPUs. When profiles are complementary (e.g., CPU-bound with I/O-bound or memory-latency–dominated tasks), the same oversubscription admits more useful overlap with less contention, improving per-host throughput. Conceptually, this variant implements parallel, capacity-ranked co-location with controlled oversubscription, preserving FIFO ordering while exposing how workload complementarity mediates contention under shared vCPU pools.

\paragraph{Co-location strategies}
\label{sec:co-location_strategies}
% Explain and visualize ShaRiff in it's variants algorihtmically and with one big visualization graphic.
% ShaRiff 1
This variant implements ShaRiff (“share resources if feasible”), which augments the FIFO pipeline with an external co-location adviser and a cluster-aware allocator. Tasks are still dequeued in strict arrival order. Before placement, the scheduler invokes ShaRiff with the current set of ready tasks and receives clusters of jobs that are predicted to co-locate well (i.e., complementary resource profiles / low expected interference). The node-assignment stage then ranks hosts by descending idle-core capacity and fills the largest host first: it forms a batch of up to that host’s idle cores and attaches the ShaRiff cluster map to the batch; if tasks remain, it proceeds to the next host in the ranked list. A small-queue fast path ensures dispatch even when only a few tasks are available.
The allocator realizes the adviser’s plan one VM per recommended cluster on the chosen host. For each multi-task cluster, it provisions a VM whose vCPU count and memory equal the sum of the clustered tasks’ declared requirements, starts the VM, and submits the tasks to that same virtual compute service. Singleton clusters are grouped into a shared VM on the host (current variant) to avoid VM fragmentation; each submitted task keeps its own job identity, and the allocator tracks VM lifecycle across all tasks assigned to it, tearing the VM down only after the last co-located task completes.
Conceptually, ShaRiff preserves FIFO ordering and capacity-ranked host filling, but replaces random batching with adviser-driven clustering. The effect is to co-locate tasks that are likely complementary (e.g., CPU-bound with I/O-bound), thereby reducing contention and improving per-host utilization without oversubscription. When the adviser yields singletons, the system still “shares if feasible” by pooling them into a shared VM, maintaining the same deterministic provisioning and lifecycle rules.

% ShaRiff 2
This variant retains the ShaRiff-augmented FIFO pipeline but enables controlled oversubscription during placement and VM sizing. Tasks are dequeued in arrival order. Before dispatch, the scheduler queries ShaRiff with the current ready set and receives clusters of tasks predicted to co-locate well (complementary resource use / low interference). Hosts are ranked by descending idle-core capacity; the assigner then fills the largest host first with a batch whose size may exceed the host’s free cores by a fixed factor (e.g., +25\%). If tasks remain, it proceeds to the next host and repeats.
The allocator implements the adviser’s plan one VM per cluster on the chosen host, but with oversubscription semantics. For multi-task clusters, it provisions a VM whose vCPU and memory equal the sum of the cluster’s requests—even if that exceeds the host’s currently free cores (hard oversub). For singleton clusters collected on the same host, it provisions a shared VM and caps vCPUs at the host’s free cores when necessary (soft cap). In both cases the VM is started once, all tasks in the cluster are submitted to the same virtual compute service, and the VM is torn down only after the last co-located task finishes.
Conceptually, this policy combines adviser-driven co-location with capacity-aware overbooking: FIFO ordering and capacity-ranked host filling are preserved, but batches may intentionally outsize instantaneous capacity to exploit latency hiding and temporal slack (e.g., I/O wait, imbalanced phases). Because ShaRiff groups complementary tasks, oversubscription tends to translate into higher throughput and energy efficiency than naive overbooking; however, when clustered tasks are less complementary, contention can surface, making this variant an explicit trade-off between utilization and interference.

% ShaRiff 3
This variant preserves FIFO dequeuing but combines round-robin first-fit placement with ShaRiff-guided intra-VM co-location. The scheduler releases tasks strictly in arrival order. The node-assignment component scans hosts in round-robin/first-fit fashion and picks the first host reporting at least one idle core. It then pulls up to that host’s idle-core capacity worth of ready tasks and queries ShaRiff for a co-location plan over this batch.
The allocator realizes ShaRiff’s plan one VM per suggested cluster on the chosen host. For each multi-task cluster, it sizes the VM by summing vCPU and memory requirements of the cluster’s tasks, starts the VM, and submits all cluster tasks to the same virtual compute service. Tasks that ShaRiff leaves as singletons are grouped onto an additional shared VM on that host; its size is the aggregate of the singletons’ requests. VM lifecycle is managed per cluster: each VM stays up until all of its assigned tasks complete, then is shut down and destroyed.
Conceptually, the policy is “first-fit host, adviser-driven packing.” It preserves FIFO ordering and simple first-fit host selection while letting ShaRiff decide which tasks should share a VM to reduce interference (by favoring complementary profiles). Unlike the max-parallel variants, this strategy does not oversubscribe cores; it fills only the currently free capacity of the first eligible host and relies on ShaRiff’s clustering to raise utilization and efficiency through informed co-location.

% MinMin extension
This scheduler extension augments the ShaRiff-based variants with a Min–Min ordering layer, a classical heuristic from list scheduling. In standard list scheduling, tasks are iteratively selected based on their earliest estimated completion time; Min–Min specifically chooses, at each step, the task (or cluster) with the minimum predicted runtime among all ready candidates and schedules it first. Here, this principle is applied not to individual tasks but to task clusters produced by ShaRiff’s co-location analysis.
For each scheduling interval, the scheduler requests from ShaRiff a co-location partition of the ready tasks, grouping them into clusters that are predicted to interact efficiently when sharing a VM. It then queries a prediction service for each cluster, obtaining runtime estimates through the chosen model (e.g., KCCA). The clusters are ordered by ascending predicted runtime, and this order defines the execution sequence. Node selection and VM provisioning are delegated to the ShaRiff node assignment and allocator components, which handle placement and resource sizing as usual.
Conceptually, this forms a Min–Min list scheduler over co-located clusters: it maintains ShaRiff’s intelligent co-location strategy while globally minimizing queueing delay and improving average completion time by prioritizing faster clusters. This layer is independent of the underlying allocation or node assignment logic and purely refines execution ordering to exploit performance prediction while preserving all structural and capacity constraints of the ShaRiff framework.