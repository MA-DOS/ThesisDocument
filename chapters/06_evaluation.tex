\section{Evaluation}
\label{cha:evaluation}

\subsection{Evaluation Setup}
\label{sec:evaluation_setup}

\subsubsection{Infrastructure}
\label{sec:evaluation_infrastructure}
% Table with different nodes and their characteristics, as well as slurm

\subsubsection{Workflows}
\label{sec:evaluation_workflows}
% Table with different workflows and their characteristics

\subsubsection{Monitoring Configuration}
\label{sec:evaluation_monitoring_configuration}
% Table with different combinations

\subsubsection{Statistical Learning Methods}
\label{sec:evaluation_statistical_learning_methods}
% Just mention that the Models are accessible on the same host via API and the parameters they were run with

\subsubsection{Simulation Configuration}
\label{sec:evaluation_simulation_configuration}
% XML file integration 

\subsection{Experiment Results}
\label{sec:experiment_results}

\subsubsection{Monitoring Statistics}
\label{sec:monitoring_statistics}

\subsubsection{Resource Contention Analysis}
\label{sec:resource_contention_analysis}
% Only summarize quickly what was done and focus on the results and their interpretation while for the approach refering to the according chapter.
% Put tables and figures in here.

\subsubsection{Statistical Learning Methods}
\label{sec:evaluation_statistical_learning_methods}

\paragraph{Task Consolidation}
\label{sec:evaluation_task_consolidation}
% Spider diagram, example task time-series that are different and clustered

\paragraph{Task Cluster Runtime and Energy Prediction}
\label{sec:evaluation_task_cluster_runtime_and_energy_prediction}
% Table with prediction results based on basic metrics

\subsubsection{Workflow Makespan and Energy Consumption}
\label{sec:workflow_makespan_and_energy_consumption}
