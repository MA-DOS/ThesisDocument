\section{Discussion}
\label{cha:discussion}
We now conclude by reflecting on the evaluation results.
% Topics
% Contention Experiments
Beginning with the experiments on resource contention, we have already discussed the observed outcomes and the underlying reasons for these behaviors. We argue that repeating these measurements on Intel-based hardware would likely yield more consistent and intuitive power consumption results for co-located benchmarks, given the better stability and accessibility of Intel's RAPL interfaces compared to AMD's.
Another factor influencing the results is the type of co-location applied is that in our setup, memory access is shared across cores via NUMA. As we confirmed the processor-to-core mapping on the test system with two CPUs available, we experimented with mapping co-located benchmark pairs according to their NUMA layout—such as pairing core 0 with 24—and pinning both tasks to the same physical CPU. Although both strategies produced broadly similar results, a more detailed investigation of the benchmark's internal performance metrics, rather than relying solely on runtime and power consumption, could yield a more accurate understanding of contention effects.
Incorporating detailed benchmark-level metrics could reduce the dependency on power-based measurements and allow the use of smaller weighting factor alpha when combining runtime and energy in the affinity computation. This refinement would likely produce more reliable and interpretable affinity scores, better reflecting the true resource interaction between co-located tasks.

% Preprocessing for clustering and prediction
The last point of discussion also relates to the results obtained from the predictor training. Overall, the evaluation indicates clear potential in using predictive models to estimate the performance and energy consumption of consolidated workflow tasks. However, several challenges remain, particularly those related to data dimensionality. The structure and representation of time-series features strongly influence how well a model can learn meaningful relationships between task behavior, runtime, and energy usage. Determining how these temporal features should be arranged, aggregated, or reduced to accurately reflect task behavior is therefore a key area for future research.
The flexibility of our monitoring approach, which records a large number of low-level features per task, also introduces complexity in feature selection. Deciding which metrics to retain and understanding how each contributes to prediction accuracy remains an open question. This uncertainty likely contributes to the observed behavior of the KCCA model, which successfully identified correlations between time-series features and performance metrics but exhibited strong overfitting.
As mentioned earlier, inconsistencies in the measured power data—stemming from the Deep-Mon fork used on AMD hardware—may further explain the instability observed in the models. Inaccurate or noisy energy readings can easily lead to model confusion, reducing generalization ability. Future work should therefore include more reliable measurement sources, improved feature preprocessing, and systematic dimensionality studies to establish a robust and interpretable prediction framework for workflow performance and energy behavior.

% WRENCH
Lastly, we discuss the results from simulating the integration of \textit{ShaReComp} into the workflow execution framework through the \textit{ShaRiff} algorithms. Although the \textit{ShaRiff} variants generally perform well and outperform the baselines, their success depends strongly on the quantity and quality of available monitoring data. As mentioned earlier, there is a direct interdependence between the effectiveness of the clustering algorithm used in \textit{ShaRiff} and the completeness of the monitoring data. This explains why, in several cases shown in the appendix, the baselines on which \textit{ShaRiff} builds outperform it when detailed monitoring information is missing for many tasks.
Our initial assumption was that combining the best-performing baseline—baseline 3—with the co-location awareness provided by \textit{ShaReComp} would consistently yield superior results. While this expectation holds for certain workflows, it does not generalize across all of them. We therefore see potential in extending the evaluation to a wider range of workflows with different task characteristics to better understand when and why \textit{ShaRiff} provides the largest benefit.
When comparing the performance of all baselines and \textit{ShaRiff} algorithms across workflows, their lower performance bound appears to remain within a similar magnitude. Several factors could explain this. The first two baselines, which do not employ co-location, clearly perform worse, as expected. However, the differences among the node assignment strategies are smaller than anticipated. For example, we expected that always selecting the largest host would result in faster completion for workflows with fewer tasks, compared to round-robin or parallel assignment strategies. Although such trends are visible, the differences are not as pronounced as predicted.
A key reason lies in the simulator design and the way baselines are implemented. During many scheduling intervals, the task queue available for allocation or co-location contained only a few tasks. To address this, we introduced a minimum queue size requirement before enabling co-location, which improved results. Nevertheless, considering task dependencies directly from the workflow DAG could further increase throughput and prevent underutilization caused by limited queue size.
We also encountered limitations in the virtual cluster compute service of WRENCH. Once tasks are assigned to a virtual machine, the VM cannot be resized dynamically; even if one task completes early, the allocated core remains idle until all tasks within that VM finish. Introducing VM resizing capabilities could significantly improve resource utilization. Similarly, instead of placing all singleton tasks that do not belong to clusters into a single VM, spawning separate VMs for each could potentially reduce makespan and energy consumption. Future work should also integrate the idle-time metric provided by SimGrid into WRENCH to quantify unused resources more precisely.
Moreover, WRENCH supports direct access to workflow DAG information from the workflow management system, which could be leveraged to improve scheduling decisions. Implementing additional random co-location and scheduling strategies would also provide more comparative baselines for assessing energy efficiency.
Overall, while \textit{ShaRiff} already demonstrates improved runtime and energy consumption in many cases, its full potential has yet to be explored. Future work should focus on integrating alternative cluster resource management strategies, expanding the host pool, and refining the platform model. In particular, moving beyond the current assumption of linear energy consumption relative to core usage toward a more fine-grained and realistic energy model would provide a more accurate evaluation of how \textit{ShaReComp} enhances ShaRiff's performance.
